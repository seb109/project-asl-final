!pip uninstall numpy -y
!pip install numpy==1.23.5
!pip install opencv-python mediapipe tensorflow numpy
import cv2
print(cv2.__version__)
import cv2
import mediapipe as mp

# Initialize MediaPipe Hand model
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.8)
mp_draw = mp.solutions.drawing_utils

# Start capturing webcam video
cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Convert frame to RGB for MediaPipe processing
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb_frame)

    # Draw hand landmarks
    if results.multi_hand_landmarks:
        for landmarks in results.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, landmarks, mp_hands.HAND_CONNECTIONS)

    cv2.imshow("Hand Tracking", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
from google.colab import output
output.enable_custom_widget_manager()




  from IPython.display import display, Javascript
from google.colab.output import eval_js

def take_photo(filename='photo.jpg', quality=0.8):
    js = f"""
    async function takePhoto() {{
      const div = document.createElement('div');
      const capture = document.createElement('button');
      capture.textContent = 'üì∏ Capture';
      div.appendChild(capture);

      const video = document.createElement('video');
      video.style.display = 'block';
      const stream = await navigator.mediaDevices.getUserMedia({{video: true}});

      document.body.appendChild(div);
      div.appendChild(video);
      video.srcObject = stream;
      await video.play();

      // Resize cell to fit video
      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

      await new Promise((resolve) => capture.onclick = resolve);
import io
import IPython.display
from PIL import Image
import base64

# Take photo (you already have this part)
photo_data = take_photo()

# Decode base64 image and open it
image_bytes = base64.b64decode(photo_data.split(',')[1])
image = Image.open(io.BytesIO(image_bytes))

# Display the image
display(image)
      const canvas = document.createElement('canvas');
      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);
      stream.getTracks().forEach(track => track.stop());
      div.remove();
      return canvas.toDataURL('image/jpeg', {quality});
    }}
    takePhoto();
    """
    return eval_js(js)



      import zipfile

with zipfile.ZipFile("aslfile.zip", 'r') as zip_ref:
    zip_ref.extractall("aslfile")


      import os

print("Top-level folders:", os.listdir("aslfile"))
data_dir = "aslfile"
print("Available gesture classes:", os.listdir(data_dir))


  import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

# Path to data
data_dir = "aslfile"

IMG_SIZE = 64
labels = os.listdir(data_dir)
labels.sort()  # Ensure consistent label order
label_map = {label: idx for idx, label in enumerate(labels)}

data = []
target = []
for label in labels:
    path = os.path.join(data_dir, label)
    for img_name in os.listdir(path)[:500]:
        img_path = os.path.join(path, img_name)

        # Skip if it's a directory
        if os.path.isdir(img_path):
            continue

        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
            data.append(img)
            target.append(label_map[label])
        else:
            print(f"Skipped unreadable image: {img_path}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential()

# Convolutional Layers
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))
model.add(MaxPooling2D(2, 2))

model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))

# Flatten and Dense Layers
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(len(labels), activation='softmax'))  # One output per class

# Compile the Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()


  # üì¶ Step 1: Imports
import os
import zipfile
import cv2
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# üóÇÔ∏è Step 2: Extract Zip (only once)
with zipfile.ZipFile("aslfile.zip", 'r') as zip_ref:
    zip_ref.extractall("asl_alphabet")

# üìÅ Step 3: Define Paths
IMG_SIZE = 64
data_dir = "asl_alphabet/asl_alphabet_train/asl_alphabet_train"  # Adjusted for nested folder

# üßº Step 4: Load and Preprocess Images
labels = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
label_map = {label: idx for idx, label in enumerate(labels)}

data = []
target = []

for label in labels:
    path = os.path.join(data_dir, label)
    files = [f for f in os.listdir(path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

    for img_name in files[:500]:  # Load up to 500 per class
        img_path = os.path.join(path, img_name)
        if os.path.isdir(img_path):
            continue
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
            data.append(img)
            target.append(label_map[label])

# ‚úÖ Confirm successful load
print(f"Loaded {len(data)} images across {len(set(target))} labels.")

# üìä Step 5: Normalize and Split
data = np.array(data) / 255.0
target = to_categorical(np.array(target))

X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=42)
print(f"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}")


model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(len(labels), activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))


import matplotlib.pyplot as plt

# Accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()


model.save("asl_model.h5")
from google.colab import drive
drive.mount('/content/drive')
pip install opencv-python mediapipe
import zipfile
import os

zip_path = "aslfile.zip"  # Make sure you‚Äôve uploaded this
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("asl_alphabet")

import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

IMG_SIZE = 64
data = []
target = []

data_dir = "asl_alphabet/asl_alphabet_train/asl_alphabet_train"
labels = sorted([f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))])
label_map = {label: idx for idx, label in enumerate(labels)}

for label in labels:
    path = os.path.join(data_dir, label)
    files = os.listdir(path)
    for img_name in files[:500]:  # Up to 500 samples per class
        img_path = os.path.join(path, img_name)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
            data.append(img)
            target.append(label_map[label])

data = np.array(data) / 255.0
target = to_categorical(np.array(target))

X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=42)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)))
model.add(MaxPooling2D(2, 2))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(2, 2))
model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(len(labels), activation='softmax'))

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

  import mediapipe as mp

from tensorflow.keras.models import load_model
model = load_model("asl_model.h5")

cap = cv2.VideoCapture(0)

labels = sorted(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',
                 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',
                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'])

mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1)
mp_draw = mp.solutions.drawing_utils

while True:
    ret, frame = cap.read()
    if not ret:
        break

    image = cv2.flip(frame, 1)
    h, w, _ = image.shape
    img_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)

    if results.multi_hand_landmarks:
        for hand_landmarks in results.multi_hand_landmarks:
            x_vals = [lm.x for lm in hand_landmarks.landmark]
            y_vals = [lm.y for lm in hand_landmarks.landmark]

            x_min = max(int(min(x_vals) * w) - 20, 0)
            y_min = max(int(min(y_vals) * h) - 20, 0)
            x_max = min(int(max(x_vals) * w) + 20, w)
            y_max = min(int(max(y_vals) * h) + 20, h)

            hand_img = image[y_min:y_max, x_min:x_max]
            if hand_img.size == 0:
                continue

            hand_img = cv2.resize(hand_img, (IMG_SIZE, IMG_SIZE))
            hand_input = np.expand_dims(hand_img / 255.0, axis=0)

            prediction = model.predict(hand_input)
            predicted_class = labels[np.argmax(prediction)]

            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
            cv2.putText(image, predicted_class, (x_min, y_min - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 0, 0), 2)
            mp_draw.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

    cv2.imshow("ASL Real-Time Recognition", image)
    if cv2.waitKey(1) & 0xFF == 27:  # ESC key
        break

cap.release()
cv2.destroyAllWindows()
  !pip install gradio

from tensorflow.keras.models import load_model
import numpy as np

model = load_model("asl_model.h5")

# Define labels (must match your training labels)
labels = sorted(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',
                 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',
                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'])

  # üõ†Ô∏è Install necessary packages
!pip install gradio mediapipe opencv-python-headless --quiet

# üì¶ Imports
import gradio as gr
import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model

# üîç Load model + labels
model = load_model("asl_model.h5")
labels = sorted(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',
                 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',
                 'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space'])

IMG_SIZE = 64
CONFIDENCE_THRESHOLD = 0.8

# ‚úã MediaPipe hand setup
mp_hands = mp.solutions.hands
hands_detector = mp_hands.Hands(static_image_mode=True, max_num_hands=1)
mp_draw = mp.solutions.drawing_utils

# üß† Main prediction function
def predict_asl(image):
    try:
        image = cv2.convertScaleAbs(image, alpha=1.3, beta=40)  # Boost contrast
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        results = hands_detector.process(image_rgb)
        h, w, _ = image.shape
        draw_overlay = image.copy()

        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # Draw landmarks on image
                mp_draw.draw_landmarks(draw_overlay, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                # Get bounding box
                x_vals = [lm.x for lm in hand_landmarks.landmark]
                y_vals = [lm.y for lm in hand_landmarks.landmark]
                x_min = max(int(min(x_vals) * w) - 20, 0)
                y_min = max(int(min(y_vals) * h) - 20, 0)
                x_max = min(int(max(x_vals) * w) + 20, w)
                y_max = min(int(max(y_vals) * h) + 20, h)

                hand_crop = image[y_min:y_max, x_min:x_max]
                if hand_crop.size == 0:
                    return draw_overlay, "üõë Hand crop failed"

                # Predict cropped hand
                img = cv2.resize(hand_crop, (IMG_SIZE, IMG_SIZE))
                img = np.expand_dims(img / 255.0, axis=0)
                preds = model.predict(img)
                top_idx = np.argmax(preds[0])
                conf = preds[0][top_idx]

                if conf < CONFIDENCE_THRESHOLD:
                    return draw_overlay, "ü§î Unsure‚Äîtry adjusting pose or lighting"

                return draw_overlay, f"‚úÖ {labels[top_idx]} ({conf*100:.1f}%)"

        # ‚ö†Ô∏è Fallback: use entire image
        fallback_img = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
        fallback_img = np.expand_dims(fallback_img / 255.0, axis=0)
        preds = model.predict(fallback_img)
        top_idx = np.argmax(preds[0])
        conf = preds[0][top_idx]

        if conf < 0.5:
            return image, "üôã No hand detected‚Äîtry again"

        return image, f"üì¶ Fallback: {labels[top_idx]} ({conf*100:.1f}%)"

    except Exception as e:
        return image, f"‚ö†Ô∏è Error: {e}"

# üöÄ Gradio interface
demo = gr.Interface(
    fn=predict_asl,
    inputs=gr.Image(type="numpy", label="Show a Sign"),
    outputs=[
        gr.Image(label="Processed Frame (with Hand Landmarks)"),
        gr.Textbox(label="Prediction")
    ],
    live=True,
    title="ASL Sign Language Recognizer (Final Version üî•)",
    description="üñêÔ∏è Real-time recognition with hand tracking, visual overlays, and fallback prediction if detection fails."
)

demo.launch(debug=True)
